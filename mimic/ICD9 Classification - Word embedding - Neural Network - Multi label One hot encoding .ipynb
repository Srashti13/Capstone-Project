{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import model_selection\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "from keras import backend\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.python.keras import backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "## Plotly\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=True)\n",
    "# Others\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.manifold import TSNE\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lsahi\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())  \n",
    "#os.chdir(r'C:\\Users\\lsahi\\Documents\\Lakamana_GMU_Sem3\\DAEN690\\mimic-iii-clinical-database-1.4')\n",
    "os.chdir(r'C:\\Users\\lsahi\\Documents\\Lakamana_GMU_Sem3\\DAEN690\\mimic-iii-clinical-database-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocess():\n",
    "    \n",
    "    def load_data():\n",
    "        NOTEEVENTS=pd.read_csv('NOTEEVENTS.csv',dtype={'ROW_ID':np.int32, 'SUBJECT_ID': np.int32,'HADM_ID': np.float64, \n",
    "                                                       'CHARTDATE':str,'STORETIME':str,'CHARTTIME':str,   \n",
    "                                                       'STORETIME': str,'CATEGORY': str,'DESCRIPTION':str,'CGID':str,'ISERROR':str,\n",
    "                                                       'TEXT':str}, parse_dates=['CHARTDATE'])\n",
    "        DIAGNOSES_ICD=pd.read_csv('DIAGNOSES_ICD.csv',dtype={'ROW_ID':np.int32, 'SUBJECT_ID': np.int32,'HADM_ID': np.int32,\n",
    "                                                             'SEQ_NUM':  np.float64,'ICD9_CODE':str})\n",
    "        DIAGNOSES_ICD['ICD9_CODE']=DIAGNOSES_ICD['ICD9_CODE'].str.pad(4,'left','0')\n",
    "        DIAGNOSES_ICD['ICD9_CHAP']=DIAGNOSES_ICD['ICD9_CODE'].str.slice(0,3)\n",
    "        DIAGNOSES_ICD=DIAGNOSES_ICD[~DIAGNOSES_ICD['ICD9_CODE'].str.slice(0,1).isin(['V','E','U','8','9'])]\n",
    "        return DIAGNOSES_ICD, NOTEEVENTS\n",
    "    \n",
    "    def diag_icd(DIAGNOSES_ICD):\n",
    "        DIAGNOSES_ICD = pd.concat([DIAGNOSES_ICD,pd.get_dummies(DIAGNOSES_ICD['ICD9_CHAP'], prefix='')],axis=1)\n",
    "        DIAGNOSES_ICD = DIAGNOSES_ICD.drop([\"ROW_ID\", \"SUBJECT_ID\", \"SEQ_NUM\", \"ICD9_CODE\", \"ICD9_CHAP\"], axis = 1)\n",
    "        b = DIAGNOSES_ICD.groupby('HADM_ID').sum()\n",
    "        b = b.replace([2,3,4,5,6,7,8,9], 1)\n",
    "        DIAGNOSES_ICD_freq=pd.DataFrame(b)\n",
    "        DIAGNOSES_ICD_freq = DIAGNOSES_ICD_freq.reset_index()\n",
    "        return DIAGNOSES_ICD_freq  \n",
    "    \n",
    "    def noteevents(NOTEEVENTS):\n",
    "        selected_doc=['Nutrition']\n",
    "        df=NOTEEVENTS[NOTEEVENTS['CATEGORY'].isin(selected_doc)].groupby('HADM_ID')['TEXT'].apply(lambda x: \"{%s}\" % ', '.join(x))\n",
    "        df2=pd.DataFrame(df)\n",
    "        df2 = df2.reset_index()\n",
    "        return df2\n",
    "    \n",
    "    def join_data(df2, DIAGNOSES_ICD_freq):\n",
    "        embed_size = 300 # how big is each word vector\n",
    "        max_features = 64763  # how many unique words to use (i.e num rows in embedding vector)\n",
    "        maxlen = 300 # max number of words in a question to use\n",
    "        NOTE_DIAGNOSES = pd.merge(df2, DIAGNOSES_ICD_freq, on = 'HADM_ID')\n",
    "        train, test = model_selection.train_test_split(NOTE_DIAGNOSES,test_size=0.2)\n",
    "        print('Size of train: '+str(train.shape[0])+' \\nSize of test: '+str(test.shape[0]) )\n",
    "        train_df, val_df = train_test_split(train, test_size=0.1, random_state=2018)\n",
    "        train_X = train_df[\"TEXT\"].fillna(\"_na_\").values\n",
    "        val_X = val_df[\"TEXT\"].fillna(\"_na_\").values\n",
    "        test_X = test[\"TEXT\"].fillna(\"_na_\").values\n",
    "\n",
    "        tokenizer = Tokenizer(num_words=max_features)\n",
    "        tokenizer.fit_on_texts(train_X)\n",
    "        train_X = tokenizer.texts_to_sequences(train_X)\n",
    "        val_X = tokenizer.texts_to_sequences(val_X)\n",
    "        test_X = tokenizer.texts_to_sequences(test_X)\n",
    "\n",
    "        train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "        val_X = pad_sequences(val_X, maxlen=maxlen)\n",
    "        test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "\n",
    "        train_y = train_df.drop(['HADM_ID', 'TEXT'], axis = 1)\n",
    "        val_y = val_df.drop(['HADM_ID', 'TEXT'], axis = 1)\n",
    "        test_y = test.drop(['HADM_ID', 'TEXT'], axis = 1)\n",
    "        return NOTE_DIAGNOSES, train_X, val_X, test_X, train_y, val_y, test_y, tokenizer\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train: 2529 \n",
      "Size of test: 633\n"
     ]
    }
   ],
   "source": [
    "DIAGNOSES_ICD, NOTEEVENTS = preprocess.load_data() \n",
    "DIAGNOSES_ICD_freq = preprocess.diag_icd(DIAGNOSES_ICD)\n",
    "df2 = preprocess.noteevents(NOTEEVENTS)\n",
    "NOTE_DIAGNOSES, train_X, val_X, test_X, train_y, val_y, test_y, tokenizer = preprocess.join_data(df2, DIAGNOSES_ICD_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class process():\n",
    "\n",
    "    def embedding(x, tokenizer):\n",
    "        embeddings_index = {}\n",
    "        # with open(r'C:\\Users\\lsahi\\Downloads\\wemb_sample.tsv', encoding = \"utf-8\") as tsvfile:\n",
    "        #           f = csv.reader('wemb_sample.tsv', delimiter='\\t')\n",
    "\n",
    "        #   for row in reader:\n",
    "        f = open(os.path.join(r'C:\\Users\\lsahi\\Downloads', x), encoding = \"utf-8\")\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:])\n",
    "            coefs = np.array(coefs, dtype=float)\n",
    "            embeddings_index[word] = coefs\n",
    "        f.close()\n",
    "        embedding_matrix = np.zeros((20000, 100))\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index > 20000 - 1:\n",
    "                break\n",
    "            else:\n",
    "                embedding_vector = embeddings_index.get(word)\n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[index] = embedding_vector\n",
    "        return embedding_matrix\n",
    "    \n",
    "    def model(embedding_matrix):\n",
    "        \n",
    "        def recall_m(y_true, y_pred):\n",
    "            true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "            possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "            recall = true_positives / (possible_positives + K.epsilon())\n",
    "            return recall\n",
    "\n",
    "        def precision_m(y_true, y_pred):\n",
    "                true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "                predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "                precision = true_positives / (predicted_positives + K.epsilon())\n",
    "                return precision\n",
    "\n",
    "        def f1_m(y_true, y_pred):\n",
    "            precision = precision_m(y_true, y_pred)\n",
    "            recall = recall_m(y_true, y_pred)\n",
    "            return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "        \n",
    "        model_glove = Sequential()\n",
    "        model_glove.add(Embedding(20000, 100, input_length=300, weights=[embedding_matrix], trainable=False))\n",
    "        model_glove.add(Dropout(0.2))\n",
    "        model_glove.add(Conv1D(64, 5, activation='relu'))\n",
    "        model_glove.add(MaxPooling1D(pool_size=4))\n",
    "        model_glove.add(LSTM(100))\n",
    "        model_glove.add(Dense(625, activation='sigmoid'))\n",
    "        model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', f1_m,precision_m, recall_m])\n",
    "\n",
    "        model_glove.fit(train_X, train_y, epochs = 2, verbose=True)\n",
    "        return model_glove\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2276/2276 [==============================] - ETA: 1:23 - loss: 0.6931 - accuracy: 0.4954 - f1_m: 0.0427 - precision_m: 0.0222 - recall_m: 0.53 - ETA: 51s - loss: 0.6924 - accuracy: 0.5522 - f1_m: 0.0415 - precision_m: 0.0217 - recall_m: 0.4742 - ETA: 41s - loss: 0.6917 - accuracy: 0.6006 - f1_m: 0.0423 - precision_m: 0.0223 - recall_m: 0.430 - ETA: 35s - loss: 0.6907 - accuracy: 0.6396 - f1_m: 0.0460 - precision_m: 0.0245 - recall_m: 0.412 - ETA: 31s - loss: 0.6896 - accuracy: 0.6723 - f1_m: 0.0466 - precision_m: 0.0250 - recall_m: 0.381 - ETA: 29s - loss: 0.6880 - accuracy: 0.6996 - f1_m: 0.0465 - precision_m: 0.0251 - recall_m: 0.354 - ETA: 27s - loss: 0.6860 - accuracy: 0.7234 - f1_m: 0.0487 - precision_m: 0.0266 - recall_m: 0.336 - ETA: 26s - loss: 0.6835 - accuracy: 0.7444 - f1_m: 0.0498 - precision_m: 0.0277 - recall_m: 0.313 - ETA: 25s - loss: 0.6797 - accuracy: 0.7626 - f1_m: 0.0501 - precision_m: 0.0283 - recall_m: 0.293 - ETA: 24s - loss: 0.6737 - accuracy: 0.7789 - f1_m: 0.0504 - precision_m: 0.0291 - recall_m: 0.274 - ETA: 23s - loss: 0.6653 - accuracy: 0.7933 - f1_m: 0.0523 - precision_m: 0.0311 - recall_m: 0.259 - ETA: 22s - loss: 0.6543 - accuracy: 0.8062 - f1_m: 0.0518 - precision_m: 0.0317 - recall_m: 0.243 - ETA: 21s - loss: 0.6420 - accuracy: 0.8181 - f1_m: 0.0525 - precision_m: 0.0341 - recall_m: 0.228 - ETA: 21s - loss: 0.6289 - accuracy: 0.8286 - f1_m: 0.0539 - precision_m: 0.0368 - recall_m: 0.217 - ETA: 20s - loss: 0.6155 - accuracy: 0.8379 - f1_m: 0.0551 - precision_m: 0.0397 - recall_m: 0.207 - ETA: 20s - loss: 0.6020 - accuracy: 0.8462 - f1_m: 0.0556 - precision_m: 0.0446 - recall_m: 0.197 - ETA: 19s - loss: 0.5886 - accuracy: 0.8536 - f1_m: 0.0562 - precision_m: 0.0493 - recall_m: 0.188 - ETA: 18s - loss: 0.5750 - accuracy: 0.8603 - f1_m: 0.0584 - precision_m: 0.0559 - recall_m: 0.181 - ETA: 18s - loss: 0.5613 - accuracy: 0.8666 - f1_m: 0.0610 - precision_m: 0.0662 - recall_m: 0.175 - ETA: 17s - loss: 0.5480 - accuracy: 0.8722 - f1_m: 0.0617 - precision_m: 0.0722 - recall_m: 0.169 - ETA: 17s - loss: 0.5348 - accuracy: 0.8772 - f1_m: 0.0635 - precision_m: 0.0866 - recall_m: 0.163 - ETA: 16s - loss: 0.5217 - accuracy: 0.8819 - f1_m: 0.0645 - precision_m: 0.0955 - recall_m: 0.158 - ETA: 16s - loss: 0.5089 - accuracy: 0.8862 - f1_m: 0.0659 - precision_m: 0.1199 - recall_m: 0.154 - ETA: 15s - loss: 0.4963 - accuracy: 0.8902 - f1_m: 0.0675 - precision_m: 0.1409 - recall_m: 0.150 - ETA: 15s - loss: 0.4840 - accuracy: 0.8939 - f1_m: 0.0691 - precision_m: 0.1615 - recall_m: 0.146 - ETA: 15s - loss: 0.4723 - accuracy: 0.8972 - f1_m: 0.0690 - precision_m: 0.1733 - recall_m: 0.142 - ETA: 14s - loss: 0.4608 - accuracy: 0.9003 - f1_m: 0.0688 - precision_m: 0.1820 - recall_m: 0.138 - ETA: 14s - loss: 0.4499 - accuracy: 0.9031 - f1_m: 0.0690 - precision_m: 0.1944 - recall_m: 0.134 - ETA: 14s - loss: 0.4392 - accuracy: 0.9058 - f1_m: 0.0696 - precision_m: 0.2071 - recall_m: 0.131 - ETA: 13s - loss: 0.4290 - accuracy: 0.9083 - f1_m: 0.0705 - precision_m: 0.2221 - recall_m: 0.128 - ETA: 13s - loss: 0.4191 - accuracy: 0.9106 - f1_m: 0.0703 - precision_m: 0.2280 - recall_m: 0.125 - ETA: 12s - loss: 0.4098 - accuracy: 0.9128 - f1_m: 0.0709 - precision_m: 0.2405 - recall_m: 0.123 - ETA: 12s - loss: 0.4007 - accuracy: 0.9148 - f1_m: 0.0708 - precision_m: 0.2464 - recall_m: 0.120 - ETA: 12s - loss: 0.3920 - accuracy: 0.9168 - f1_m: 0.0715 - precision_m: 0.2576 - recall_m: 0.118 - ETA: 11s - loss: 0.3840 - accuracy: 0.9185 - f1_m: 0.0717 - precision_m: 0.2672 - recall_m: 0.116 - ETA: 11s - loss: 0.3759 - accuracy: 0.9203 - f1_m: 0.0714 - precision_m: 0.2702 - recall_m: 0.114 - ETA: 11s - loss: 0.3683 - accuracy: 0.9219 - f1_m: 0.0713 - precision_m: 0.2738 - recall_m: 0.112 - ETA: 10s - loss: 0.3610 - accuracy: 0.9235 - f1_m: 0.0716 - precision_m: 0.2814 - recall_m: 0.110 - ETA: 10s - loss: 0.3541 - accuracy: 0.9249 - f1_m: 0.0719 - precision_m: 0.2886 - recall_m: 0.108 - ETA: 10s - loss: 0.3473 - accuracy: 0.9264 - f1_m: 0.0720 - precision_m: 0.2924 - recall_m: 0.107 - ETA: 9s - loss: 0.3409 - accuracy: 0.9277 - f1_m: 0.0720 - precision_m: 0.2967 - recall_m: 0.105 - ETA: 9s - loss: 0.3348 - accuracy: 0.9290 - f1_m: 0.0725 - precision_m: 0.3037 - recall_m: 0.10 - ETA: 9s - loss: 0.3290 - accuracy: 0.9301 - f1_m: 0.0725 - precision_m: 0.3090 - recall_m: 0.10 - ETA: 8s - loss: 0.3234 - accuracy: 0.9313 - f1_m: 0.0722 - precision_m: 0.3112 - recall_m: 0.10 - ETA: 8s - loss: 0.3181 - accuracy: 0.9323 - f1_m: 0.0706 - precision_m: 0.3043 - recall_m: 0.09 - ETA: 8s - loss: 0.3130 - accuracy: 0.9334 - f1_m: 0.0691 - precision_m: 0.2977 - recall_m: 0.09 - ETA: 7s - loss: 0.3079 - accuracy: 0.9344 - f1_m: 0.0676 - precision_m: 0.2914 - recall_m: 0.09 - ETA: 7s - loss: 0.3030 - accuracy: 0.9354 - f1_m: 0.0662 - precision_m: 0.2853 - recall_m: 0.09 - ETA: 7s - loss: 0.2985 - accuracy: 0.9362 - f1_m: 0.0649 - precision_m: 0.2795 - recall_m: 0.09 - ETA: 6s - loss: 0.2942 - accuracy: 0.9371 - f1_m: 0.0636 - precision_m: 0.2739 - recall_m: 0.08 - ETA: 6s - loss: 0.2900 - accuracy: 0.9379 - f1_m: 0.0623 - precision_m: 0.2685 - recall_m: 0.08 - ETA: 6s - loss: 0.2860 - accuracy: 0.9387 - f1_m: 0.0611 - precision_m: 0.2634 - recall_m: 0.08 - ETA: 5s - loss: 0.2820 - accuracy: 0.9395 - f1_m: 0.0600 - precision_m: 0.2584 - recall_m: 0.08 - ETA: 5s - loss: 0.2781 - accuracy: 0.9403 - f1_m: 0.0588 - precision_m: 0.2536 - recall_m: 0.08 - ETA: 5s - loss: 0.2744 - accuracy: 0.9410 - f1_m: 0.0578 - precision_m: 0.2490 - recall_m: 0.08 - ETA: 4s - loss: 0.2709 - accuracy: 0.9416 - f1_m: 0.0567 - precision_m: 0.2445 - recall_m: 0.07 - ETA: 4s - loss: 0.2674 - accuracy: 0.9423 - f1_m: 0.0568 - precision_m: 0.2474 - recall_m: 0.07 - ETA: 4s - loss: 0.2641 - accuracy: 0.9430 - f1_m: 0.0570 - precision_m: 0.2507 - recall_m: 0.07 - ETA: 3s - loss: 0.2609 - accuracy: 0.9436 - f1_m: 0.0570 - precision_m: 0.2528 - recall_m: 0.07 - ETA: 3s - loss: 0.2578 - accuracy: 0.9442 - f1_m: 0.0571 - precision_m: 0.2558 - recall_m: 0.07 - ETA: 3s - loss: 0.2546 - accuracy: 0.9448 - f1_m: 0.0577 - precision_m: 0.2614 - recall_m: 0.07 - ETA: 2s - loss: 0.2518 - accuracy: 0.9453 - f1_m: 0.0578 - precision_m: 0.2652 - recall_m: 0.07 - ETA: 2s - loss: 0.2489 - accuracy: 0.9459 - f1_m: 0.0584 - precision_m: 0.2709 - recall_m: 0.07 - ETA: 2s - loss: 0.2462 - accuracy: 0.9464 - f1_m: 0.0590 - precision_m: 0.2775 - recall_m: 0.07 - ETA: 1s - loss: 0.2435 - accuracy: 0.9470 - f1_m: 0.0602 - precision_m: 0.2806 - recall_m: 0.07 - ETA: 1s - loss: 0.2410 - accuracy: 0.9474 - f1_m: 0.0612 - precision_m: 0.2840 - recall_m: 0.07 - ETA: 1s - loss: 0.2385 - accuracy: 0.9479 - f1_m: 0.0625 - precision_m: 0.2886 - recall_m: 0.07 - ETA: 0s - loss: 0.2359 - accuracy: 0.9484 - f1_m: 0.0633 - precision_m: 0.2896 - recall_m: 0.07 - ETA: 0s - loss: 0.2337 - accuracy: 0.9488 - f1_m: 0.0641 - precision_m: 0.2927 - recall_m: 0.07 - ETA: 0s - loss: 0.2314 - accuracy: 0.9492 - f1_m: 0.0651 - precision_m: 0.2954 - recall_m: 0.07 - ETA: 0s - loss: 0.2291 - accuracy: 0.9497 - f1_m: 0.0660 - precision_m: 0.2981 - recall_m: 0.07 - 23s 10ms/step - loss: 0.2288 - accuracy: 0.9497 - f1_m: 0.0658 - precision_m: 0.2957 - recall_m: 0.0739\n",
      "Epoch 2/2\n",
      "2276/2276 [==============================] - ETA: 18s - loss: 0.0667 - accuracy: 0.9816 - f1_m: 0.0891 - precision_m: 0.5625 - recall_m: 0.048 - ETA: 17s - loss: 0.0675 - accuracy: 0.9812 - f1_m: 0.0851 - precision_m: 0.5469 - recall_m: 0.046 - ETA: 17s - loss: 0.0679 - accuracy: 0.9811 - f1_m: 0.0810 - precision_m: 0.5208 - recall_m: 0.043 - ETA: 16s - loss: 0.0684 - accuracy: 0.9807 - f1_m: 0.0736 - precision_m: 0.4766 - recall_m: 0.039 - ETA: 16s - loss: 0.0683 - accuracy: 0.9807 - f1_m: 0.0776 - precision_m: 0.5063 - recall_m: 0.042 - ETA: 16s - loss: 0.0675 - accuracy: 0.9809 - f1_m: 0.0807 - precision_m: 0.5208 - recall_m: 0.043 - ETA: 16s - loss: 0.0669 - accuracy: 0.9812 - f1_m: 0.0783 - precision_m: 0.5000 - recall_m: 0.042 - ETA: 16s - loss: 0.0680 - accuracy: 0.9807 - f1_m: 0.0777 - precision_m: 0.5078 - recall_m: 0.042 - ETA: 16s - loss: 0.0681 - accuracy: 0.9806 - f1_m: 0.0690 - precision_m: 0.4514 - recall_m: 0.037 - ETA: 15s - loss: 0.0686 - accuracy: 0.9803 - f1_m: 0.0621 - precision_m: 0.4062 - recall_m: 0.033 - ETA: 15s - loss: 0.0688 - accuracy: 0.9803 - f1_m: 0.0565 - precision_m: 0.3693 - recall_m: 0.030 - ETA: 15s - loss: 0.0687 - accuracy: 0.9803 - f1_m: 0.0518 - precision_m: 0.3385 - recall_m: 0.028 - ETA: 14s - loss: 0.0686 - accuracy: 0.9803 - f1_m: 0.0478 - precision_m: 0.3125 - recall_m: 0.025 - ETA: 14s - loss: 0.0685 - accuracy: 0.9803 - f1_m: 0.0444 - precision_m: 0.2902 - recall_m: 0.024 - ETA: 14s - loss: 0.0685 - accuracy: 0.9802 - f1_m: 0.0414 - precision_m: 0.2708 - recall_m: 0.022 - ETA: 13s - loss: 0.0686 - accuracy: 0.9801 - f1_m: 0.0388 - precision_m: 0.2539 - recall_m: 0.021 - ETA: 13s - loss: 0.0687 - accuracy: 0.9800 - f1_m: 0.0365 - precision_m: 0.2390 - recall_m: 0.019 - ETA: 13s - loss: 0.0684 - accuracy: 0.9802 - f1_m: 0.0345 - precision_m: 0.2257 - recall_m: 0.018 - ETA: 13s - loss: 0.0687 - accuracy: 0.9800 - f1_m: 0.0327 - precision_m: 0.2138 - recall_m: 0.017 - ETA: 12s - loss: 0.0684 - accuracy: 0.9802 - f1_m: 0.0311 - precision_m: 0.2031 - recall_m: 0.016 - ETA: 12s - loss: 0.0685 - accuracy: 0.9801 - f1_m: 0.0320 - precision_m: 0.2098 - recall_m: 0.017 - ETA: 12s - loss: 0.0687 - accuracy: 0.9801 - f1_m: 0.0336 - precision_m: 0.2216 - recall_m: 0.018 - ETA: 12s - loss: 0.0688 - accuracy: 0.9801 - f1_m: 0.0350 - precision_m: 0.2310 - recall_m: 0.018 - ETA: 12s - loss: 0.0692 - accuracy: 0.9800 - f1_m: 0.0343 - precision_m: 0.2630 - recall_m: 0.018 - ETA: 11s - loss: 0.0693 - accuracy: 0.9799 - f1_m: 0.0329 - precision_m: 0.2525 - recall_m: 0.017 - ETA: 11s - loss: 0.0691 - accuracy: 0.9800 - f1_m: 0.0316 - precision_m: 0.2428 - recall_m: 0.017 - ETA: 11s - loss: 0.0691 - accuracy: 0.9800 - f1_m: 0.0305 - precision_m: 0.2338 - recall_m: 0.016 - ETA: 11s - loss: 0.0690 - accuracy: 0.9800 - f1_m: 0.0328 - precision_m: 0.2489 - recall_m: 0.017 - ETA: 10s - loss: 0.0690 - accuracy: 0.9799 - f1_m: 0.0344 - precision_m: 0.2597 - recall_m: 0.018 - ETA: 10s - loss: 0.0689 - accuracy: 0.9800 - f1_m: 0.0359 - precision_m: 0.2688 - recall_m: 0.019 - ETA: 10s - loss: 0.0689 - accuracy: 0.9800 - f1_m: 0.0369 - precision_m: 0.2732 - recall_m: 0.019 - ETA: 10s - loss: 0.0687 - accuracy: 0.9800 - f1_m: 0.0385 - precision_m: 0.2832 - recall_m: 0.020 - ETA: 9s - loss: 0.0686 - accuracy: 0.9801 - f1_m: 0.0397 - precision_m: 0.2898 - recall_m: 0.021 - ETA: 9s - loss: 0.0687 - accuracy: 0.9800 - f1_m: 0.0407 - precision_m: 0.2969 - recall_m: 0.02 - ETA: 9s - loss: 0.0686 - accuracy: 0.9800 - f1_m: 0.0423 - precision_m: 0.3063 - recall_m: 0.02 - ETA: 9s - loss: 0.0683 - accuracy: 0.9801 - f1_m: 0.0436 - precision_m: 0.3125 - recall_m: 0.02 - ETA: 8s - loss: 0.0685 - accuracy: 0.9801 - f1_m: 0.0444 - precision_m: 0.3184 - recall_m: 0.02 - ETA: 8s - loss: 0.0685 - accuracy: 0.9800 - f1_m: 0.0454 - precision_m: 0.3265 - recall_m: 0.02 - ETA: 8s - loss: 0.0684 - accuracy: 0.9800 - f1_m: 0.0461 - precision_m: 0.3301 - recall_m: 0.02 - ETA: 8s - loss: 0.0683 - accuracy: 0.9801 - f1_m: 0.0471 - precision_m: 0.3344 - recall_m: 0.02 - ETA: 7s - loss: 0.0683 - accuracy: 0.9801 - f1_m: 0.0475 - precision_m: 0.3369 - recall_m: 0.02 - ETA: 7s - loss: 0.0683 - accuracy: 0.9801 - f1_m: 0.0483 - precision_m: 0.3415 - recall_m: 0.02 - ETA: 7s - loss: 0.0684 - accuracy: 0.9801 - f1_m: 0.0489 - precision_m: 0.3452 - recall_m: 0.02 - ETA: 7s - loss: 0.0683 - accuracy: 0.9801 - f1_m: 0.0503 - precision_m: 0.3544 - recall_m: 0.02 - ETA: 6s - loss: 0.0683 - accuracy: 0.9801 - f1_m: 0.0510 - precision_m: 0.3597 - recall_m: 0.02 - ETA: 6s - loss: 0.0684 - accuracy: 0.9801 - f1_m: 0.0513 - precision_m: 0.3607 - recall_m: 0.02 - ETA: 6s - loss: 0.0685 - accuracy: 0.9800 - f1_m: 0.0521 - precision_m: 0.3684 - recall_m: 0.02 - ETA: 6s - loss: 0.0685 - accuracy: 0.9800 - f1_m: 0.0524 - precision_m: 0.3698 - recall_m: 0.02 - ETA: 5s - loss: 0.0684 - accuracy: 0.9800 - f1_m: 0.0528 - precision_m: 0.3724 - recall_m: 0.02 - ETA: 5s - loss: 0.0684 - accuracy: 0.9800 - f1_m: 0.0536 - precision_m: 0.3781 - recall_m: 0.02 - ETA: 5s - loss: 0.0684 - accuracy: 0.9800 - f1_m: 0.0541 - precision_m: 0.3805 - recall_m: 0.02 - ETA: 5s - loss: 0.0684 - accuracy: 0.9800 - f1_m: 0.0541 - precision_m: 0.3804 - recall_m: 0.02 - ETA: 4s - loss: 0.0684 - accuracy: 0.9800 - f1_m: 0.0548 - precision_m: 0.3856 - recall_m: 0.02 - ETA: 4s - loss: 0.0684 - accuracy: 0.9800 - f1_m: 0.0553 - precision_m: 0.3883 - recall_m: 0.02 - ETA: 4s - loss: 0.0683 - accuracy: 0.9800 - f1_m: 0.0553 - precision_m: 0.3881 - recall_m: 0.02 - ETA: 3s - loss: 0.0685 - accuracy: 0.9800 - f1_m: 0.0563 - precision_m: 0.3962 - recall_m: 0.03 - ETA: 3s - loss: 0.0684 - accuracy: 0.9800 - f1_m: 0.0569 - precision_m: 0.3997 - recall_m: 0.03 - ETA: 3s - loss: 0.0684 - accuracy: 0.9800 - f1_m: 0.0570 - precision_m: 0.4003 - recall_m: 0.03 - ETA: 3s - loss: 0.0684 - accuracy: 0.9800 - f1_m: 0.0571 - precision_m: 0.4010 - recall_m: 0.03 - ETA: 2s - loss: 0.0684 - accuracy: 0.9800 - f1_m: 0.0575 - precision_m: 0.4036 - recall_m: 0.03 - ETA: 2s - loss: 0.0684 - accuracy: 0.9800 - f1_m: 0.0576 - precision_m: 0.4037 - recall_m: 0.03 - ETA: 2s - loss: 0.0684 - accuracy: 0.9800 - f1_m: 0.0581 - precision_m: 0.4073 - recall_m: 0.03 - ETA: 2s - loss: 0.0683 - accuracy: 0.9800 - f1_m: 0.0585 - precision_m: 0.4097 - recall_m: 0.03 - ETA: 1s - loss: 0.0684 - accuracy: 0.9800 - f1_m: 0.0591 - precision_m: 0.4141 - recall_m: 0.03 - ETA: 1s - loss: 0.0683 - accuracy: 0.9800 - f1_m: 0.0595 - precision_m: 0.4168 - recall_m: 0.03 - ETA: 1s - loss: 0.0683 - accuracy: 0.9800 - f1_m: 0.0598 - precision_m: 0.4181 - recall_m: 0.03 - ETA: 1s - loss: 0.0681 - accuracy: 0.9800 - f1_m: 0.0596 - precision_m: 0.4156 - recall_m: 0.03 - ETA: 0s - loss: 0.0680 - accuracy: 0.9801 - f1_m: 0.0599 - precision_m: 0.4168 - recall_m: 0.03 - ETA: 0s - loss: 0.0680 - accuracy: 0.9801 - f1_m: 0.0602 - precision_m: 0.4180 - recall_m: 0.03 - ETA: 0s - loss: 0.0680 - accuracy: 0.9801 - f1_m: 0.0605 - precision_m: 0.4196 - recall_m: 0.03 - ETA: 0s - loss: 0.0680 - accuracy: 0.9801 - f1_m: 0.0606 - precision_m: 0.4203 - recall_m: 0.03 - 19s 8ms/step - loss: 0.0680 - accuracy: 0.9801 - f1_m: 0.0605 - precision_m: 0.4214 - recall_m: 0.0327\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = process.embedding('wemb_sample.tsv', tokenizer)\n",
    "model_glove = process.model(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
